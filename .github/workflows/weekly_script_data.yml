name: Weekly Data Scraping and Storage

on:
  schedule:
    - cron: "0 0 * * 0"  # Run every Sunday at midnight UTC
  workflow_dispatch:  # Enable manual triggering
    inputs:
      message:
        description: 'Run on manual triggering'
        required: false

jobs:
  scrape_and_store_data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10.11'  # Specify the Python version

      - name: Install CUDA Toolkit
        run: |
          # Add the NVIDIA CUDA repository key
          sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub
          
          # Add the NVIDIA CUDA repository to the package sources
          sudo sh -c 'echo "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64 /" > /etc/apt/sources.list.d/cuda.list'
          
          # Update the package lists
          sudo apt-get update
          
          # Install CUDA Toolkit
          sudo apt-get install -y cuda

      - name: Install System Dependencies
        run: |
          sudo apt-get install -y cmake swig build-essential

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --upgrade tensorflow keras
          pip install --no-deps --no-cache-dir -r requirements.txt  # Install project dependencies

      - name: Install Azure CLI
        uses: azure/CLI@v1
        with:
          inlineScript: |
            # Your Azure CLI commands go here
            az --version

      - name: Execute data scraping script
        env:
          DATA_FOLDER: ${{ github.workspace }}/src/data
        run: python src/data_scraping.py

      - name: Upload data to Azure Blob Storage
        run: |
          az storage blob upload \
            --connection-string "${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}" \
            --container-name stock-prediction-data \
            --file src/data/StockPriceDataWebScraped.parquet  # Corrected file path
            --name StockPriceData.parquet \
            --overwrite