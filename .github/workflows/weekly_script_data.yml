name: Weekly Data Scraping and Storage

on:
  schedule:
    - cron: "0 0 * * 0"  # Run every Sunday at midnight UTC
  workflow_dispatch:  # Enable manual triggering
    inputs:
      message:
        description: 'Run on manual triggering'
        required: false

jobs:
  scrape_and_store_data:
    runs-on: ubuntu-latest
    container:
      image: nvidia/cuda:12.4.1-base-ubuntu20.04  # Use a Docker image with CUDA support

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Install Git
        run: |
          apt-get update && apt-get install -y git

      - name: Install Build Tools
        run: |
          apt-get update && apt-get install -y build-essential

      - name: Install SWIG
        run: |
          apt-get update && apt-get install -y swig

      - name: Install CMake
        run: |
          export DEBIAN_FRONTEND=noninteractive
          apt-get update && apt-get install -y cmake --no-install-recommends

      - name: Install Python Development Headers
        run: |
          apt-get update && apt-get install -y python3.10-dev 

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10.11'  # Specify the Python version

      - name: Create Virtual Environment
        run: |
          python -m venv venv

      - name: Activate Virtual Environment
        run: |
          bash -c "source venv/bin/activate"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --upgrade tensorflow keras --user
          pip install --no-deps --no-cache-dir -r requirements.txt --user  # Install project dependencies

      - name: Install Azure CLI
        uses: azure/CLI@v1
        with:
          inlineScript: |
            # Your Azure CLI commands go here
            az --version

      - name: Execute data scraping script
        env:
          DATA_FOLDER: ${{ github.workspace }}/src/data
        run: python src/data_scraping.py

      - name: Upload data to Azure Blob Storage
        run: |
          az storage blob upload \
            --connection-string "${{ secrets.AZURE_STORAGE_CONNECTION_STRING }}" \
            --container-name stock-prediction-data \
            --file src/data/StockPriceDataWebScraped.parquet  # Corrected file path
            --name StockPriceData.parquet \
            --overwrite
